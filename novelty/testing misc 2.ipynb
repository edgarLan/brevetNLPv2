{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\edgar\\OneDrive\\Bureau\\Ecole\\HEC\\A24\\BrevetNLP\\.conda\\Lib\\site-packages\\spacy\\util.py:910: UserWarning: [W095] Model 'en_core_web_sm' (3.8.0) was trained with spaCy v3.8.0 and may not be 100% compatible with the current version (3.7.5). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\edgar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<module 'surprise' from 'c:\\\\Users\\\\edgar\\\\OneDrive\\\\Bureau\\\\Ecole\\\\HEC\\\\A24\\\\BrevetNLP\\\\PatentNovelty\\\\novelty\\\\surprise.py'>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from os import walk\n",
    "import json\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from utils import pmi, pmi_to_dict_adj_dict, docs_distribution, new_distribution, OptimizedIncrementalPMI\n",
    "from scoring import compute_scores\n",
    "import time\n",
    "import pandas as pd\n",
    "from textCleaning import get_file_names, extract_year_ipc\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "import utils\n",
    "from utils import docs_distribution, new_distribution, combine_columns, OptimizedIncrementalPMI\n",
    "import copy\n",
    "from novelty import ClusterKS\n",
    "\n",
    "import importlib\n",
    "importlib.reload(utils)\n",
    "from utils import docs_distribution, new_distribution, combine_columns, OptimizedIncrementalPMI\n",
    "import scoring\n",
    "importlib.reload(scoring)\n",
    "from scoring import compute_scores\n",
    "import novelty\n",
    "importlib.reload(novelty)\n",
    "import surprise\n",
    "importlib.reload(surprise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_size = 3\n",
    "path = \"/home/edgarlanoue/data/csvCleaned/\" #\"C:/Users/edgar/OneDrive/Bureau/Ecole/HEC/A24/BrevetNLP/exemple données/cleaned/\"\n",
    "pathOutput = \"/home/edgarlanoue/metrics/\" #\"C:/Users/edgar/OneDrive/Bureau/Ecole/HEC/A24/BrevetNLP/exemple données/metrics/\"\n",
    "path = \"C:/Users/edgar/OneDrive/Bureau/Ecole/HEC/A24/BrevetNLP/exemple données/cleaned/\"\n",
    "pathOutput = \"C:/Users/edgar/OneDrive/Bureau/Ecole/HEC/A24/BrevetNLP/exemple données/metrics/\"\n",
    "\n",
    "useClusters=True\n",
    "ipcList = [\"H01L\"]\n",
    "yearList = [\"2016\"]\n",
    "tE_cols = [\"claims\"]\n",
    "base_cols = [\"claims\", \"background\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016\n",
      "    H01L\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 284544/284544 [01:15<00:00, 3779.13it/s]\n"
     ]
    }
   ],
   "source": [
    "# def measureNov(path, pathOutput, tE_cols, base_cols, w_size, useClusters, yearList, ipcList):\n",
    "start_time = time.time()  # Record the starting time\n",
    "if yearList == [] or ipcList == []:\n",
    "    tE_names = get_file_names(path+\"tE\")\n",
    "    KS_names = get_file_names(path+\"KS\")\n",
    "    ES_names = get_file_names(path+\"ES\")\n",
    "\n",
    "    tE_set = set([extract_year_ipc(string) for string in tE_names])\n",
    "    KS_set = set([extract_year_ipc(string) for string in KS_names])\n",
    "    ES_set = set([extract_year_ipc(string) for string in ES_names])\n",
    "\n",
    "    assert(tE_set==KS_set==ES_set)\n",
    "\n",
    "    yearList = sorted(list(set([year for year, ipc in list(tE_set)])))\n",
    "    ipcList = list(set([ipc for year, ipc in list(tE_set)]))\n",
    "    print(yearList)\n",
    "    print(ipcList)\n",
    "\n",
    "for year in yearList:\n",
    "    for ipc in ipcList:\n",
    "        print(f\"{year}\")\n",
    "        print(f\"    {ipc}\")\n",
    "        tE = pd.read_csv(path+f\"tE/{year}_{ipc}_tE_cleaned.csv\")\n",
    "        KS = pd.read_csv(path+f\"KS/{year}_{ipc}_KS_cleaned.csv\")\n",
    "        ES = pd.read_csv(path+f\"ES/{year}_{ipc}_ES_cleaned.csv\")\n",
    "\n",
    "        ### Transforming KB into distribution\n",
    "        application_number = tE[\"application_number\"]\n",
    "        label = tE[\"label\"]\n",
    "\n",
    "        tE=combine_columns(tE, tE_cols) #[:10000]\n",
    "        KS=combine_columns(KS, base_cols) #[:100000]\n",
    "        ES=combine_columns(ES, base_cols) #[:100000]\n",
    "\n",
    "        # Création pmi ES\n",
    "        # current_time = time.time()  # Get the current time\n",
    "        # elapsed_time = current_time - start_time\n",
    "        # start_time = current_time\n",
    "        # print(elapsed_time)\n",
    "\n",
    "        # print(\"ES\")\n",
    "        base_texts = [word for text in tqdm(ES) for word in text.split()]\n",
    "        ES_PMI = OptimizedIncrementalPMI(3)\n",
    "        ES_PMI.update(base_texts)\n",
    "        instance_ES_pmi = ES_PMI.compute_pmi()\n",
    "        base_bigram_set = set(instance_ES_pmi.keys())\n",
    "        \n",
    "        dict_ES_pmi = pmi_to_dict_adj_dict(instance_ES_pmi)\n",
    "        # print(\"ES finidhsed\")\n",
    "        \n",
    "        # # current_time = time.time()  # Get the current time\n",
    "        # # elapsed_time = current_time - start_time\n",
    "        # # start_time = current_time\n",
    "        # # print(elapsed_time)\n",
    "\n",
    "        # # print(\"KS\")\n",
    "        # KS_matrix, KS_dist, KS_Count_matrix = docs_distribution(baseSpace=KS, tE=tE)\n",
    "        # KS_size = list(range(KS_matrix.shape[0]))\n",
    "        # # print(\"KS finished\")\n",
    "\n",
    "        # # current_time = time.time()  # Get the current time\n",
    "        # # elapsed_time = current_time - start_time\n",
    "        # # start_time = current_time\n",
    "        # # print(elapsed_time)\n",
    "\n",
    "        # # print(\"Cluster\")\n",
    "        # if useClusters==True:\n",
    "        #     KSClusterDiff1000 = ClusterKS(list_know_P=KS_matrix, new_Q= None, N=100, nbPtsPerCluster=1000)\n",
    "        #     KSClusterDiff1000.clusterKS()\n",
    "        # # print(\"Cluster finished\")    \n",
    "        # # ES_size = list(range(ES_matrix.shape[0]))\n",
    "        \n",
    "        # # current_time = time.time()  # Get the current time\n",
    "        # # elapsed_time = current_time - start_time\n",
    "        # # start_time = current_time\n",
    "        # # print(elapsed_time)\n",
    "        \n",
    "        # ## We set to 0 the distance here for each recipe -- difference needs to estimate distance between all points. \n",
    "        # #This serves as optim to not calculate for each varaitions but only once since it is the same distance for all KB\n",
    "        # # print('train variations for recipe {} done'.format(recette), ' | train variation size : ', len(train_recettes))\n",
    "        \n",
    "        # neighborhood_distance  = 0.\n",
    "        # new_ratio_vec = []\n",
    "        # new_bin_vec = []\n",
    "        # uniq_ratio_vec = []\n",
    "        # uniq_bin_vec = []\n",
    "        # diff_ratio_vec = []\n",
    "        # diff_bin_vec = []\n",
    "        # neighborhood_distance_vec = []\n",
    "        # surpDiv_ratio_vec = []\n",
    "        # surpDiv_bin_vec = []\n",
    "        # # mean100_vec = []\n",
    "        \n",
    "        # for i in len(tE):\n",
    "        #     # print(\"new KS dist (with 1 toEval)\")\n",
    "        #     select_variation = KS_size + [len(KS_size)+i]\n",
    "        #     NewKS_dist, variation_dist = new_distribution(KS_Count_matrix, select_variation)\n",
    "            \n",
    "        #     # print(\"new ES pmi (with 1 toEval)\")\n",
    "        #     # # instance_ES_updated_pmi = copy.deepcopy(incremental_pmi_ES_only) # reprise du pmi du ES seulement\n",
    "        #     # new_pmi_instance = OptimizedIncrementalPMI(window_size = w_size)\n",
    "        #     # update_text = [word for text in tqdm(tE[i]) for word in text.split()] \n",
    "        #     # new_pmi_instance.update(update_text)\n",
    "        #     # new_pmi = new_pmi_instance.compute_pmi()\n",
    "\n",
    "        #     baseTexts_update = [tE[i]]\n",
    "        #     update_text = [word for text in (baseTexts_update) for word in text.split()]\n",
    "        #     new_pmi = OptimizedIncrementalPMI(window_size=3)\n",
    "        #     new_pmi.update(update_text)\n",
    "        #     newpmi_PMI = new_pmi.compute_pmi()\n",
    "            \n",
    "        #     # print(\"compute scores\")\n",
    "        #     results = compute_scores(KB_matrix=KS_matrix, KB_dist=KS_dist, NewKB_dist=NewKS_dist, variation_dist=variation_dist, \n",
    "        #                                 EB_PMI=instance_ES_pmi, base_bigram_set=base_bigram_set, dict_know_pmi=dict_ES_pmi, New_EB_PMI=newpmi_PMI,\n",
    "        #                                 neighbor_dist=neighborhood_distance, useClusters=True, KSCluster=KSClusterDiff1000)\n",
    "\n",
    "        #     new_ratio = results[0]\n",
    "        #     new_bin = results[1]\n",
    "\n",
    "        #     uniq_ratio = results[2]\n",
    "        #     uniq_bin = results[3]\n",
    "\n",
    "        #     diff_ratio = results[4]\n",
    "        #     diff_bin = results[5]\n",
    "        #     neighborhood_distance = results[6] #update neighborhood distance so it isn't reset\n",
    "        #     # mean100 = results[7]\n",
    "\n",
    "        #     # surpNew_ratio = results[7]\n",
    "        #     # surpNew_bin = results[8]\n",
    "\n",
    "        #     surpDiv_ratio = results[8]\n",
    "        #     surpDiv_bin = results[9]\n",
    "\n",
    "        #     new_ratio_vec.append(new_ratio)\n",
    "        #     new_bin_vec.append(new_bin)\n",
    "\n",
    "        #     uniq_ratio_vec.append(uniq_ratio)\n",
    "        #     uniq_bin_vec.append(uniq_bin)\n",
    "\n",
    "        #     diff_ratio_vec.append(diff_ratio)\n",
    "        #     diff_bin_vec.append(diff_bin)\n",
    "        #     neighborhood_distance_vec.append(neighborhood_distance)\n",
    "        #     # mean100_vec.append(mean100)\n",
    "\n",
    "        #     surpDiv_ratio_vec.append(surpDiv_ratio)\n",
    "        #     surpDiv_bin_vec.append(surpDiv_bin)\n",
    "\n",
    "        #     if i % 1000 == 0:\n",
    "        #         current_time = time.time()  # Get the current time\n",
    "        #         elapsed_time = current_time - start_time  # Calculate elapsed time\n",
    "        #         if i % 1000 == 0:\n",
    "        #             print(f\"{i} on {len(tE)}, time since last print: {elapsed_time:.2f} seconds\")\n",
    "        #         start_time = current_time  # Reset the start time for the next interval\n",
    "        #             # Reset the start time to measure time per iteration\n",
    "        #         start_time = time.time()\n",
    "\n",
    "        # # print(\"df\")\n",
    "\n",
    "        \n",
    "        # start_time = current_time\n",
    "        # df = pd.DataFrame({\n",
    "        #     \"application_number\": application_number[:2],\n",
    "        #     \"label\": label[:2],\n",
    "\n",
    "        #     \"new_ratio\": new_ratio_vec,\n",
    "        #     \"new_bin\": new_bin_vec,\n",
    "\n",
    "        #     \"uniq_ratio\": uniq_ratio_vec,\n",
    "        #     \"uniq_bin\": uniq_bin_vec,\n",
    "\n",
    "        #     \"diff_ratio\": diff_ratio_vec,\n",
    "        #     \"diff_bin\": diff_bin_vec, \n",
    "        #     \"neighboroud_distance\": neighborhood_distance_vec,\n",
    "        #     # \"mean100\": mean100_vec\n",
    "\n",
    "        #     # \"surpNew_ratio\": surpNew_ratio,\n",
    "        #     # \"surpNew_bin\": surpNew_bin,\n",
    "\n",
    "        #     \"surpDiv_ratio\":surpDiv_ratio_vec,\n",
    "        #     \"surpDiv_bin\": surpDiv_bin_vec\n",
    "        # })\n",
    "        # # current_time = time.time()  # Get the current time\n",
    "        # # elapsed_time = current_time - start_time\n",
    "        # # print(elapsed_time)\n",
    "\n",
    "        # tE_cols_str = \"_\".join(tE_cols)\n",
    "        # base_cols_str = \"_\".join(base_cols)\n",
    "        # df.to_csv(pathOutput + f'/{year}_{ipc}_{tE_cols_str}_vs_{base_cols_str}_Metrics.csv', index=False)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_texts = [word for text in (ES) for word in text.split()]\n",
    "\n",
    "# ES_PMI.update(base_texts)\n",
    "# instance_ES_pmi = ES_PMI.compute_pmi()\n",
    "# base_bigram_set = set(instance_ES_pmi.keys())\n",
    "\n",
    "# dict_ES_pmi = pmi_to_dict_adj_dict(instance_ES_pmi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "del base_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "100000\n",
      "110000\n",
      "120000\n",
      "130000\n",
      "140000\n",
      "150000\n",
      "160000\n",
      "170000\n",
      "180000\n",
      "190000\n",
      "200000\n",
      "210000\n",
      "220000\n",
      "230000\n",
      "240000\n",
      "250000\n",
      "260000\n",
      "270000\n",
      "280000\n"
     ]
    }
   ],
   "source": [
    "chunk_size = 10000  # Adjust based on available memory\n",
    "ES_PMI_2 = OptimizedIncrementalPMI(3)\n",
    "\n",
    "for i in range(0, len(ES), chunk_size):\n",
    "    # base_texts = (word for text in ES[i:i+chunk_size] for word in text.split())  # Generator\n",
    "    base_texts = [word for text in (ES[i:i+chunk_size]) for word in text.split()]\n",
    "    ES_PMI_2.update(base_texts)\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "instance_ES_pmi_2 = ES_PMI_2.compute_pmi()\n",
    "base_bigram_set_2 = set(instance_ES_pmi_2.keys())\n",
    "\n",
    "dict_ES_pmi_2 = pmi_to_dict_adj_dict(instance_ES_pmi_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_ES_pmi_2 == dict_ES_pmi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "108188"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dict_ES_pmi_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "108188"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dict_ES_pmi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_ES_pmi_2.keys() == dict_ES_pmi.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_ES_pmi_2.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
