{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\edgar\\OneDrive\\Bureau\\Ecole\\HEC\\A24\\BrevetNLP\\.conda\\Lib\\site-packages\\spacy\\util.py:910: UserWarning: [W095] Model 'en_core_web_sm' (3.8.0) was trained with spaCy v3.8.0 and may not be 100% compatible with the current version (3.7.5). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\edgar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<module 'novelty' from 'c:\\\\Users\\\\edgar\\\\OneDrive\\\\Bureau\\\\Ecole\\\\HEC\\\\A24\\\\BrevetNLP\\\\PatentNovelty\\\\novelty\\\\novelty.py'>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from os import walk\n",
    "import json\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from utils import pmi, pmi_to_dict_adj, docs_distribution, new_distribution, OptimizedIncrementalPMI\n",
    "from scoring import compute_scores\n",
    "import time\n",
    "import pandas as pd\n",
    "from textCleaning import get_file_names, extract_year_ipc\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "import utils\n",
    "from utils import docs_distribution, new_distribution, combine_columns, OptimizedIncrementalPMI\n",
    "import copy\n",
    "from novelty import ClusterKS\n",
    "\n",
    "import importlib\n",
    "importlib.reload(utils)\n",
    "from utils import docs_distribution, new_distribution, combine_columns, OptimizedIncrementalPMI\n",
    "import scoring\n",
    "importlib.reload(scoring)\n",
    "from scoring import compute_scores\n",
    "import novelty\n",
    "importlib.reload(novelty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['summary', 'abstract', 'background']\n",
    "w_size = 3\n",
    "path = \"/home/edgarlanoue/data/csvCleaned/\" #\"C:/Users/edgar/OneDrive/Bureau/Ecole/HEC/A24/BrevetNLP/exemple données/cleaned/\"\n",
    "pathOutput = \"/home/edgarlanoue/metrics/\" #\"C:/Users/edgar/OneDrive/Bureau/Ecole/HEC/A24/BrevetNLP/exemple données/metrics/\"\n",
    "useClusters=True\n",
    "yearList = []\n",
    "ipcList = []\n",
    "\n",
    "def measureNov(path, pathOutput, columns, w_size, useClusters, yearList, ipcList):\n",
    "    \n",
    "    start_time = time.time()  # Record the starting time\n",
    "    if yearList == [] or ipcList == []:\n",
    "        tE_names = get_file_names(path+\"tE\")\n",
    "        KS_names = get_file_names(path+\"KS\")\n",
    "        ES_names = get_file_names(path+\"ES\")\n",
    "    \n",
    "        tE_set = set([extract_year_ipc(string) for string in tE_names])\n",
    "        KS_set = set([extract_year_ipc(string) for string in KS_names])\n",
    "        ES_set = set([extract_year_ipc(string) for string in ES_names])\n",
    "    \n",
    "        assert(tE_set==KS_set==ES_set)\n",
    "    \n",
    "        yearList = sorted(list(set([year for year, ipc in list(tE_set)])))\n",
    "        ipcList = list(set([ipc for year, ipc in list(tE_set)]))\n",
    "        print(yearList)\n",
    "        print(ipcList)\n",
    "    \n",
    "    for year in yearList:\n",
    "        for ipc in ipcList:\n",
    "            print(f\"{year}\")\n",
    "            print(f\"    {ipc}\")\n",
    "            tE = pd.read_csv(path+f\"tE/{year}_{ipc}_tE_cleaned.csv\")\n",
    "            KS = pd.read_csv(path+f\"KS/{year}_{ipc}_KS_cleaned.csv\")\n",
    "            # ES = pd.read_csv(path+f\"ES/{year}_{ipc}_ES_cleaned.csv\")\n",
    "    \n",
    "            # patent = tE[k]\n",
    "            # file_path = my_path + recette\n",
    "            # with open(file_path) as json_file:\n",
    "            #     recipe_dict = json.load(json_file)\n",
    "    \n",
    "            #### COLLECTING ALL NECESSARY INFO\n",
    "    \n",
    "            # KB_recettes, _ = data_analysis(recipe_dict)\n",
    "            # print('Knowledge base size : ', len(KB_recettes))\n",
    "            # if len(KB_recettes) <= 0:\n",
    "            #     continue\n",
    "    \n",
    "            ### Transforming KB into distribution\n",
    "            application_number = tE[\"application_number\"]\n",
    "            label = tE[\"label\"]\n",
    "    \n",
    "            tE=combine_columns(tE, columns)\n",
    "            KS=combine_columns(KS, columns)\n",
    "            # ES=combine_columns(ES, columns)\n",
    "    \n",
    "            # Création pmi ES\n",
    "            # ES_PMI = pmi(ES)\n",
    "            \n",
    "            # base_texts = [word for text in tqdm(ES) for word in text.split()]\n",
    "            # instance_ES_pmi = OptimizedIncrementalPMI(window_size = w_size)\n",
    "            # print(\"pmi\")\n",
    "            # instance_ES_pmi.update(base_texts)\n",
    "            # ES_pmi = instance_ES_pmi.compute_pmi()\n",
    "            \n",
    "            # dict_ES_pmi = pmi_to_dict_adj(ES_pmi[0])\n",
    "            \n",
    "            KS_matrix, KS_dist, KS_Count_matrix = docs_distribution(baseSpace=KS, tE=tE)\n",
    "            KS_size = list(range(KS_matrix.shape[0]))\n",
    "            if useClusters==True:\n",
    "                KSClusterDiff1000 = ClusterKS(list_know_P=KS_matrix, new_Q= None, N=100, nbPtsPerCluster=1000)\n",
    "                KSClusterDiff1000.clusterKS()\n",
    "    \n",
    "            # ES_matrix,  ES_dist, ES_Count_matrix = docs_distribution(baseSpace=ES, tE=tE, columns=columns)\n",
    "            # ES_size = list(range(ES_matrix.shape[0]))\n",
    "            \n",
    "            ## We set to 0 the distance here for each recipe -- difference needs to estimate distance between all points. \n",
    "            #This serves as optim to not calculate for each varaitions but only once since it is the same distance for all KB\n",
    "            # print('train variations for recipe {} done'.format(recette), ' | train variation size : ', len(train_recettes))\n",
    "            \n",
    "            neighborhood_distance  = 0.\n",
    "            new_ratio_vec = []\n",
    "            new_bin_vec = []\n",
    "            uniq_ratio_vec = []\n",
    "            uniq_bin_vec = []\n",
    "            diff_ratio_vec = []\n",
    "            diff_bin_vec = []\n",
    "            neighborhood_distance_vec = []\n",
    "            # mean100_vec = []\n",
    "    \n",
    "            for i in (range(len(tE))):\n",
    "                # print(\"new KS dist (with 1 toEval)\")\n",
    "                select_variation = KS_size + [len(KS_size)+i]\n",
    "                NewKS_dist, variation_dist = new_distribution(KS_Count_matrix, select_variation)\n",
    "                \n",
    "                # print(\"new ES pmi (with 1 toEval)\")\n",
    "                # # instance_ES_updated_pmi = copy.deepcopy(incremental_pmi_ES_only) # reprise du pmi du ES seulement\n",
    "                # new_pmi_instance = OptimizedIncrementalPMI(window_size = w_size)\n",
    "                # update_text = [word for text in tqdm(tE[i]) for word in text.split()] \n",
    "                # new_pmi_instance.update(update_text)\n",
    "                # new_pmi = new_pmi_instance.compute_pmi()\n",
    "    \n",
    "    \n",
    "                # print(\"compute scores\")\n",
    "                results = compute_scores(KS_matrix, KS_dist, NewKS_dist, variation_dist,# ES_pmi, dict_ES_pmi, new_pmi,\n",
    "                                            neighbor_dist=neighborhood_distance, useClusters=True, KSCluster=KSClusterDiff1000)\n",
    "    \n",
    "                new_ratio = results[0]\n",
    "                new_bin = results[1]\n",
    "    \n",
    "                uniq_ratio = results[2]\n",
    "                uniq_bin = results[3]\n",
    "    \n",
    "                diff_ratio = results[4]\n",
    "                diff_bin = results[5]\n",
    "                neighborhood_distance = results[6] #update neighborhood distance so it isn't reset\n",
    "                # mean100 = results[7]\n",
    "    \n",
    "                # surpNew_ratio = results[7]\n",
    "                # surpNew_bin = results[8]\n",
    "    \n",
    "                # surpDiv_ratio = results[9]\n",
    "                # surpDiv_bin = results[10]\n",
    "    \n",
    "                new_ratio_vec.append(new_ratio)\n",
    "                new_bin_vec.append(new_bin)\n",
    "    \n",
    "                uniq_ratio_vec.append(uniq_ratio)\n",
    "                uniq_bin_vec.append(uniq_bin)\n",
    "    \n",
    "                diff_ratio_vec.append(diff_ratio)\n",
    "                diff_bin_vec.append(diff_bin)\n",
    "                neighborhood_distance_vec.append(neighborhood_distance)\n",
    "                # mean100_vec.append(mean100)\n",
    "    \n",
    "                if i % 1000 == 0:\n",
    "                    current_time = time.time()  # Get the current time\n",
    "                    elapsed_time = current_time - start_time  # Calculate elapsed time\n",
    "                    if i % 1000 == 0:\n",
    "                        print(f\"{i} on {len(tE)}, time since last print: {elapsed_time:.2f} seconds\")\n",
    "                    start_time = current_time  # Reset the start time for the next interval\n",
    "                        # Reset the start time to measure time per iteration\n",
    "                    start_time = time.time()\n",
    "    \n",
    "    \n",
    "            df = pd.DataFrame({\n",
    "                \"application_number\": application_number,\n",
    "                \"label\": label,\n",
    "    \n",
    "                \"new_ratio\": new_ratio_vec,\n",
    "                \"new_bin\": new_bin_vec,\n",
    "    \n",
    "                \"uniq_ratio\": uniq_ratio_vec,\n",
    "                \"uniq_bin\": uniq_bin_vec,\n",
    "    \n",
    "                \"diff_ratio\": diff_ratio_vec,\n",
    "                \"diff_bin\": diff_bin_vec, \n",
    "                \"neighboroud_distance\": neighborhood_distance_vec,\n",
    "                # \"mean100\": mean100_vec\n",
    "    \n",
    "                # \"surpNew_ratio\": surpNew_ratio,\n",
    "                # \"surpNew_bin\": surpNew_bin,\n",
    "    \n",
    "                # \"surpDiv_ratio\":surpDiv_ratio,\n",
    "                # \"surpDiv_bin\": surpDiv_bin\n",
    "            })\n",
    "            df.to_csv(pathOutput + f'/{year}_{ipc}_{columns[0]}_noveltyMetrics.csv', index=False)\n",
    "            \n",
    "\n",
    "measureNov(path, pathOutput, columns, w_size, useClusters, yearList, ipcList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"C:/Users/edgar/OneDrive/Bureau/Ecole/HEC/A24/BrevetNLP/exemple données/cleaned/\"\n",
    "# path = \"/home/edgarlanoue/data/csvCleaned/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016\n",
      "    H01L\n"
     ]
    }
   ],
   "source": [
    "tE_names = get_file_names(path+\"tE\")\n",
    "KS_names = get_file_names(path+\"KS\")\n",
    "ES_names = get_file_names(path+\"ES\")\n",
    "\n",
    "tE_set = set([extract_year_ipc(string) for string in tE_names])\n",
    "KS_set = set([extract_year_ipc(string) for string in KS_names])\n",
    "ES_set = set([extract_year_ipc(string) for string in ES_names])\n",
    "\n",
    "assert(tE_set==KS_set==ES_set)\n",
    "\n",
    "yearList = [year for year, ipc in list(tE_set)]\n",
    "ipcList = [ipc for year, ipc in list(tE_set)]\n",
    "\n",
    "yearList = [\"2016\"]\n",
    "ipcList = [\"H01L\"]\n",
    "\n",
    "for year in yearList:\n",
    "    for ipc in ipcList:\n",
    "        print(f\"{year}\")\n",
    "        print(f\"    {ipc}\")\n",
    "        tE = pd.read_csv(path+f\"tE/{year}_{ipc}_tE_cleaned.csv\")\n",
    "        KS = pd.read_csv(path+f\"KS/{year}_{ipc}_KS_cleaned.csv\")\n",
    "        ES = pd.read_csv(path+f\"ES/{year}_{ipc}_ES_cleaned.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import utils\n",
    "importlib.reload(utils)\n",
    "from utils import docs_distribution, new_distribution, combine_columns, OptimizedIncrementalPMI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "application_number = tE[\"application_number\"]\n",
    "label = tE[\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tE=combine_columns(tE, [\"claims\"])\n",
    "KS=combine_columns(KS, [\"claims\"])\n",
    "ES=combine_columns(ES, [\"claims\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import utils\n",
    "importlib.reload(utils)\n",
    "from utils import docs_distribution, new_distribution, combine_columns, OptimizedIncrementalPMI, pmi\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "284544"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 284544/284544 [00:27<00:00, 10331.46it/s]\n"
     ]
    }
   ],
   "source": [
    "# base_texts = ' '.join(ES[:1000]).split()\n",
    "base_texts = [word for text in tqdm(ES) for word in text.split()]\n",
    "# ES_pmi = pmi(base_texts, w_size=3)\n",
    "# ES_pmi\n",
    "\n",
    "# 38s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pmi le mien vs défaut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11075532/11075532 [00:31<00:00, 354886.16it/s]\n"
     ]
    }
   ],
   "source": [
    "instOptPMI = OptimizedIncrementalPMI(3)\n",
    "instOptPMI.update(base_texts)\n",
    "dict_optPMI = instOptPMI.compute_pmi()\n",
    "# 8min 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4014866"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dict_optPMI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PMI, les deux sont pareils à 1 secondes prêts... le mien est utile puisqu'il segmente les taches, rendant un PMI futur beaucoup plus rapide à calculer, cependant, pu besoin puisqu'on compare simplement l'intersection entre l'ES et le nouveau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 382.62it/s]\n"
     ]
    }
   ],
   "source": [
    "baseTexts_update = [tE[1]]\n",
    "update_text = [word for text in tqdm(baseTexts_update) for word in text.split()] #' '.join(baseTexts_update).split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 155/155 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "new_pmi = OptimizedIncrementalPMI(window_size=3)\n",
    "new_pmi.update(update_text)\n",
    "newpmi_PMI = new_pmi.compute_pmi()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # base_texts_set = set(base_texts)\n",
    "# # update_text_set = set(update_text)\n",
    "\n",
    "# base_bigram_set = set(instOptPMI.bigram_counts.keys())\n",
    "\n",
    "# update_bigram_set = set(new_pmi.bigram_counts.keys())\n",
    "\n",
    "# common_bigram_set = update_bigram_set & base_bigram_set\n",
    "# # common_word_set = update_text_set & base_texts_set\n",
    "\n",
    "\n",
    "# import math\n",
    "\n",
    "# # 20aine de secondes pour trouver ce qu'ils ont en commun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_bigram_set = set(instOptPMI.bigram_counts.keys())\n",
    "\n",
    "# update_bigram_set = set(new_pmi.bigram_counts.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_bigram_set2 = set(dict_optPMI.keys())\n",
    "# update_bigram_set2 = set(newpmi_PMI.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "# common_bigram_set2 = update_bigram_set2 & base_bigram_set2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "import surprise\n",
    "from surprise import Surprise\n",
    "from utils import pmi_to_dict_adj_dict\n",
    "instSurp = Surprise(newpmi_PMI)\n",
    "instSurp2 = Surprise(newpmi_PMI)\n",
    "importlib.reload(utils)\n",
    "from utils import pmi_to_dict_adj_dict, dict2mat\n",
    "importlib.reload(surprise)\n",
    "from surprise import Surprise\n",
    "from utils import pmi_to_dict_adj_dict\n",
    "instSurp = Surprise(newpmi_PMI)\n",
    "instSurp2 = Surprise(newpmi_PMI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w1:  84621\n",
      "w2:  84621\n"
     ]
    }
   ],
   "source": [
    "dict_known = pmi_to_dict_adj_dict(dict_optPMI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "from divergences import Jensen_Shannon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Version longue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unique_surp_longue(newpmi_PMI, dict_known):\n",
    "    dict_new = pmi_to_dict_adj_dict(newpmi_PMI)\n",
    "    # print(\"get_common_vectors\")\n",
    "    vecotr_tuples = instSurp.get_common_vectors_adj(dict_known, dict_new, epsilon = 0)\n",
    "\n",
    "    key_list = vecotr_tuples.keys()\n",
    "    surprise_dists = []\n",
    "    # print(key_list)\n",
    "    # print(\"JSDiv\")\n",
    "    len_surpDist=0\n",
    "    #i=0\n",
    "    for entry in (key_list):\n",
    "        tuple_known, tuple_new = vecotr_tuples[entry]\n",
    "        \n",
    "        #### We want only positive PMI score -- no negative values for not going nan or inf values  \n",
    "\n",
    "        mask = (np.array(tuple_known) != 0).astype(int)\n",
    "        tuple_known = np.maximum(0., np.array(tuple_known))\n",
    "        # Apply the mask to tuple_known\n",
    "        tuple_new = tuple_new * mask\n",
    "\n",
    "        tuple_known = np.maximum(0., np.array(tuple_known))\n",
    "        # if i==4:\n",
    "        #     print(list(tuple_known))\n",
    "        #     print(sum(tuple_known))\n",
    "        #     print(\"FINITO\")\n",
    "        #     print(tuple_new)\n",
    "        #     print(sum(tuple_new))\n",
    "        # i+=1\n",
    "        mask = (np.array(tuple_new) != 0).astype(int)\n",
    "        tuple_new = np.maximum(0., np.array(tuple_new))\n",
    "        # Apply the mask to tuple_known\n",
    "        tuple_known = tuple_known * mask\n",
    "\n",
    "        if tuple_new.sum():\n",
    "            len_surpDist+=1\n",
    "            if tuple_known.sum():\n",
    "                surprise_dists.append(Jensen_Shannon().JSDiv(tuple_known, tuple_new))\n",
    "            else: surprise_dists.append(0)\n",
    "\n",
    "    surprise_score = sum(surprise_dists) / len_surpDist # 2 etant la différence entre nb intersection, et nb nouveau\n",
    "\n",
    "    return surprise_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Version courte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unique_surp_courte(newpmi_PMI, base_bigram_set2):\n",
    "    update_bigram_set2 = set(newpmi_PMI.keys())\n",
    "    common_bigram_set2 = update_bigram_set2 & base_bigram_set2\n",
    "\n",
    "    dict_new2 = pmi_to_dict_adj_dict({key: newpmi_PMI[key] for key in common_bigram_set2})\n",
    "    dict_known2 = pmi_to_dict_adj_dict({key: dict_optPMI[key] for key in common_bigram_set2})\n",
    "    vecotr_tuples2 = instSurp2.get_common_vectors_adj(dict_known2, dict_new2, epsilon = 0)\n",
    "\n",
    "    key_list = vecotr_tuples2.keys()\n",
    "    surprise_dists = []\n",
    "\n",
    "    for entry in (key_list):\n",
    "        tuple_known2, tuple_new2 = vecotr_tuples2[entry]\n",
    "        \n",
    "        #### We want only positive PMI score -- no negative values for not going nan or inf values  \n",
    "        mask = (np.array(tuple_known2) != 0).astype(int)\n",
    "        tuple_known2 = np.maximum(0., np.array(tuple_known2))\n",
    "        # Apply the mask to tuple_known\n",
    "        tuple_new2 = tuple_new2 * mask\n",
    "\n",
    "        mask = (np.array(tuple_new2) != 0).astype(int)\n",
    "        tuple_new2 = np.maximum(0., np.array(tuple_new2))\n",
    "        # Apply the mask to tuple_known\n",
    "        tuple_known2 = tuple_known2 * mask\n",
    "\n",
    "        if tuple_known2.sum() and tuple_new2.sum():\n",
    "            surprise_dists.append(Jensen_Shannon().JSDiv(tuple_known2, tuple_new2))\n",
    "            # print(Jensen_Shannon().JSDiv(tuple_known2, tuple_new2))\n",
    "        else: surprise_dists.append(0)\n",
    "    surprise_score2 = sum(surprise_dists) / len(surprise_dists)\n",
    "\n",
    "    return surprise_score2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Version mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unique_surp_mat(newpmi_PMI, base_bigram_set2):\n",
    "    update_bigram_set2 = set(newpmi_PMI.keys())\n",
    "    common_bigram_set2 = update_bigram_set2 & base_bigram_set2\n",
    "\n",
    "    mat_old = dict2mat({key: dict_optPMI[key] for key in common_bigram_set2})\n",
    "    mat_new = dict2mat({key: newpmi_PMI[key] for key in common_bigram_set2})\n",
    "\n",
    "    surprise_dists3 = []\n",
    "    # print(key_list)\n",
    "    # print(\"JSDiv\")\n",
    "    # i=0\n",
    "    for entry in (mat_old.columns):\n",
    "        tuple_known3, tuple_new3 = mat_old[entry], mat_new[entry]\n",
    "        \n",
    "        #### We want only positive PMI score -- no negative values for not going nan or inf values  \n",
    "        mask = (np.array(tuple_known3) != 0).astype(int)\n",
    "        tuple_known3 = np.maximum(0., np.array(tuple_known3))\n",
    "        # Apply the mask to tuple_known\n",
    "        tuple_new3 = tuple_new3 * mask\n",
    "\n",
    "        # tuple_known3 = np.maximum(0., np.array(tuple_known3))\n",
    "        # if i==4:\n",
    "        #     print(list(tuple_known3))\n",
    "        #     print(sum(tuple_known3))\n",
    "        #     print(\"FINITO\")\n",
    "        #     print(list(tuple_new3))\n",
    "        #     print(sum(tuple_new3))\n",
    "        # i+=1\n",
    "        mask = (np.array(tuple_new3) != 0).astype(int)\n",
    "        tuple_new3 = np.maximum(0., np.array(tuple_new3))\n",
    "        # Apply the mask to tuple_known\n",
    "        tuple_known3 = tuple_known3 * mask\n",
    "\n",
    "        if tuple_known3.sum() and tuple_new3.sum():\n",
    "            surprise_dists3.append(Jensen_Shannon().JSDiv(tuple_known3, tuple_new3))\n",
    "            # print(Jensen_Shannon().JSDiv(tuple_known3, tuple_new3))\n",
    "        else: surprise_dists3.append(0)\n",
    "\n",
    "    surprise_score3 = sum(surprise_dists3) / (len(surprise_dists3))\n",
    "    return surprise_score3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.15938992446016972"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "surprise_score = unique_surp_longue(newpmi_PMI, dict_known)\n",
    "surprise_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.15938992446016975"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "surprise_score2 = unique_surp_courte(newpmi_PMI, base_bigram_set2)\n",
    "surprise_score2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.15938992446016975"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "surprise_score3 = unique_surp_mat(newpmi_PMI, base_bigram_set2)\n",
    "surprise_score3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "func1 execution time: 4.101536 seconds\n",
      "func2 execution time: 0.169670 seconds\n",
      "func3 execution time: 0.706946 seconds\n",
      "func2 is the fastest.\n"
     ]
    }
   ],
   "source": [
    "import timeit\n",
    "\n",
    "# Define wrapper functions to pass the arguments\n",
    "def wrapper_func1():\n",
    "    unique_surp_longue(newpmi_PMI, dict_known)\n",
    "\n",
    "def wrapper_func2():\n",
    "    unique_surp_courte(newpmi_PMI, base_bigram_set2)\n",
    "\n",
    "def wrapper_func3():\n",
    "    unique_surp_mat(newpmi_PMI, base_bigram_set2)\n",
    "\n",
    "# Measure execution time for each function\n",
    "time_func1 = timeit.timeit(wrapper_func1, number=100)  # Run 10,000 iterations\n",
    "time_func2 = timeit.timeit(wrapper_func2, number=100)\n",
    "time_func3 = timeit.timeit(wrapper_func3, number=100)\n",
    "\n",
    "# Print results\n",
    "print(f\"func1 execution time: {time_func1:.6f} seconds\")\n",
    "print(f\"func2 execution time: {time_func2:.6f} seconds\")\n",
    "print(f\"func3 execution time: {time_func3:.6f} seconds\")\n",
    "\n",
    "# Determine the fastest function\n",
    "fastest = min(time_func1, time_func2, time_func3)\n",
    "if fastest == time_func1:\n",
    "    print(\"func1 is the fastest.\")\n",
    "elif fastest == time_func2:\n",
    "    print(\"func2 is the fastest.\")\n",
    "else:\n",
    "    print(\"func3 is the fastest.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09356191140192403"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "surprise_score2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09356191140192403"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "surprise_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "import surprise\n",
    "from surprise import Surprise\n",
    "importlib.reload(utils)\n",
    "importlib.reload(surprise)\n",
    "from utils import pmi_to_dict_adj_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "instSurp = Surprise(newpmi_PMI)\n",
    "from utils import pmi_to_dict_adj_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_known = pmi_to_dict_adj_dict(dict_optPMI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45962"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dict_known)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_common_vectors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [00:00<00:00, 11834.15it/s]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from divergences import Jensen_Shannon "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSDiv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5196458649677629"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key_list = vecotr_tuples.keys()\n",
    "surprise_dists = []\n",
    "# print(key_list)\n",
    "print(\"JSDiv\")\n",
    "#i=0\n",
    "for entry in tqdm(key_list):\n",
    "    tuple_known, tuple_new = vecotr_tuples[entry]\n",
    "    \n",
    "    #### We want only positive PMI score -- no negative values for not going nan or inf values  \n",
    "    tuple_known = np.maximum(0., np.array(tuple_known))\n",
    "    # if i==4:\n",
    "    #     print(list(tuple_known))\n",
    "    #     print(sum(tuple_known))\n",
    "    #     print(\"FINITO\")\n",
    "    #     print(tuple_new)\n",
    "    #     print(sum(tuple_new))\n",
    "    # i+=1\n",
    "    tuple_new = np.maximum(0., np.array(tuple_new))\n",
    "\n",
    "    if tuple_known.sum() and tuple_new.sum():\n",
    "        surprise_dists.append(Jensen_Shannon().JSDiv(tuple_known, tuple_new))\n",
    "    else: surprise_dists.append(0)\n",
    "\n",
    "surprise_score = sum(surprise_dists) / len(surprise_dists)\n",
    "\n",
    "surprise_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5251444058852863"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "surprise_score"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
