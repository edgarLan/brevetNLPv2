{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'surprise' from 'c:\\\\Users\\\\edgar\\\\OneDrive\\\\Bureau\\\\Ecole\\\\HEC\\\\A24\\\\BrevetNLP\\\\PatentNovelty\\\\novelty\\\\surprise.py'>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from os import walk\n",
    "import json\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from utils import pmi, pmi_to_dict_adj_dict, docs_distribution, new_distribution, OptimizedIncrementalPMI\n",
    "from scoring import compute_scores\n",
    "import time\n",
    "import pandas as pd\n",
    "from textCleaning import get_file_names, extract_year_ipc\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "import utils\n",
    "from utils import docs_distribution, new_distribution, combine_columns, OptimizedIncrementalPMI\n",
    "import copy\n",
    "from novelty import ClusterKS\n",
    "\n",
    "import importlib\n",
    "importlib.reload(utils)\n",
    "from utils import docs_distribution, new_distribution, combine_columns, OptimizedIncrementalPMI\n",
    "import scoring\n",
    "importlib.reload(scoring)\n",
    "from scoring import compute_scores\n",
    "import novelty\n",
    "importlib.reload(novelty)\n",
    "import surprise\n",
    "importlib.reload(surprise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"C:/Users/edgar/OneDrive/Bureau/Ecole/HEC/A24/BrevetNLP/exemple données/cleaned/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "year=2012\n",
    "ipc=\"H01L\"\n",
    "tE_cols = [\"abstract\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "tE = pd.read_csv(path+f\"tE/{year}_{ipc}_tE_cleaned.csv\")\n",
    "tE=combine_columns(tE, tE_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'novel fluoroacyl arylamines'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tE[1227]\n",
    "tE[7547]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_size = 3\n",
    "path = \"/home/edgarlanoue/data/csvCleaned/\" #\"C:/Users/edgar/OneDrive/Bureau/Ecole/HEC/A24/BrevetNLP/exemple données/cleaned/\"\n",
    "pathOutput = \"/home/edgarlanoue/metrics/\" #\"C:/Users/edgar/OneDrive/Bureau/Ecole/HEC/A24/BrevetNLP/exemple données/metrics/\"\n",
    "path = \"C:/Users/edgar/OneDrive/Bureau/Ecole/HEC/A24/BrevetNLP/exemple données/cleaned/\"\n",
    "pathOutput = \"C:/Users/edgar/OneDrive/Bureau/Ecole/HEC/A24/BrevetNLP/exemple données/metrics/\"\n",
    "\n",
    "useClusters=True\n",
    "ipcList = [\"H01L\"]\n",
    "yearList = [\"2012\"]\n",
    "tE_cols = [\"abstract\"]\n",
    "base_cols = [\"abstract\", \"summary\", \"background\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/1000 [00:01<30:43,  1.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7000 on 19198, time since last print: 116.81 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▍    | 547/1000 [15:42<13:00,  1.72s/it]\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 98\u001b[0m\n\u001b[0;32m     95\u001b[0m newpmi_PMI \u001b[38;5;241m=\u001b[39m new_pmi\u001b[38;5;241m.\u001b[39mcompute_pmi()\n\u001b[0;32m     97\u001b[0m \u001b[38;5;66;03m# print(\"compute scores\")\u001b[39;00m\n\u001b[1;32m---> 98\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_scores\u001b[49m\u001b[43m(\u001b[49m\u001b[43mKB_matrix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mKS_matrix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mKB_dist\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mKS_dist\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mNewKB_dist\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mNewKS_dist\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvariation_dist\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvariation_dist\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     99\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mEB_PMI\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minstance_ES_pmi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbase_bigram_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_bigram_set\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdict_know_pmi\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdict_ES_pmi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mNew_EB_PMI\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnewpmi_PMI\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    100\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mneighbor_dist\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mneighborhood_distance\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43museClusters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mKSCluster\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mKSClusterDiff1000\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    102\u001b[0m new_ratio \u001b[38;5;241m=\u001b[39m results[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    103\u001b[0m new_bin \u001b[38;5;241m=\u001b[39m results[\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\edgar\\OneDrive\\Bureau\\Ecole\\HEC\\A24\\BrevetNLP\\PatentNovelty\\novelty\\scoring.py:43\u001b[0m, in \u001b[0;36mcompute_scores\u001b[1;34m(KB_matrix, KB_dist, NewKB_dist, variation_dist, dict_know_pmi, EB_PMI, base_bigram_set, New_EB_PMI, newness_type, uniq_type, diff_type, neighbor_dist, useClusters, KSCluster)\u001b[0m\n\u001b[0;32m     41\u001b[0m surprise \u001b[38;5;241m=\u001b[39m Surprise(New_EB_PMI)\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m# newratio_surprise_rate, newn_suprise = surprise.new_surprise(EB_PMI, thr_surp=0.0104)\u001b[39;00m\n\u001b[1;32m---> 43\u001b[0m dist_surprise, uniq_surprise \u001b[38;5;241m=\u001b[39m \u001b[43msurprise\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munique_surp_courte\u001b[49m\u001b[43m(\u001b[49m\u001b[43mNew_EB_PMI\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mEB_PMI\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbase_bigram_set\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.00\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthr_surp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.00256\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m newness, novelty_new, uniqueness, novelty_uniq, dif_score, dif_bin, neighbor_dist, mean100, dist_surprise, uniq_surprise\n",
      "File \u001b[1;32mc:\\Users\\edgar\\OneDrive\\Bureau\\Ecole\\HEC\\A24\\BrevetNLP\\PatentNovelty\\novelty\\surprise.py:195\u001b[0m, in \u001b[0;36mSurprise.unique_surp_courte\u001b[1;34m(self, newpmi_PMI, known_pmi, base_bigram_set, eps, thr_surp)\u001b[0m\n\u001b[0;32m    193\u001b[0m         surprise_dists\u001b[38;5;241m.\u001b[39mappend(Jensen_Shannon()\u001b[38;5;241m.\u001b[39mJSDiv(tuple_known, tuple_new))\n\u001b[0;32m    194\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m: surprise_dists\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m--> 195\u001b[0m surprise_score \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msurprise_dists\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msurprise_dists\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    196\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m surprise_score \u001b[38;5;241m>\u001b[39m thr_surp:\n\u001b[0;32m    197\u001b[0m     dist_surprise \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[1;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "# start_time = time.time()  # Record the starting time\n",
    "\n",
    "\n",
    "# for year in yearList:\n",
    "#     for ipc in ipcList:\n",
    "#         print(f\"{year}\")\n",
    "#         print(f\"    {ipc}\")\n",
    "#         tE = pd.read_csv(path+f\"tE/{year}_{ipc}_tE_cleaned.csv\")\n",
    "#         KS = pd.read_csv(path+f\"KS/{year}_{ipc}_KS_cleaned.csv\")\n",
    "#         ES = pd.read_csv(path+f\"ES/{year}_{ipc}_ES_cleaned.csv\")\n",
    "\n",
    "#         ### Transforming KB into distribution\n",
    "#         application_number = tE[\"application_number\"]\n",
    "#         label = tE[\"label\"]\n",
    "\n",
    "#         tE=combine_columns(tE, tE_cols) #[:10000]\n",
    "#         KS=combine_columns(KS, base_cols) #[:100000]\n",
    "#         ES=combine_columns(ES, base_cols) #[:100000]\n",
    "\n",
    "#         # Création pmi ES\n",
    "#         # current_time = time.time()  # Get the current time\n",
    "#         # elapsed_time = current_time - start_time\n",
    "#         # start_time = current_time\n",
    "#         # print(elapsed_time)\n",
    "\n",
    "#         # print(\"ES\")\n",
    "#         base_texts = [word for text in (ES) for word in text.split()]\n",
    "#         ES_PMI = OptimizedIncrementalPMI(3)\n",
    "#         ES_PMI.update(base_texts)\n",
    "#         instance_ES_pmi = ES_PMI.compute_pmi()\n",
    "#         base_bigram_set = set(instance_ES_pmi.keys())\n",
    "        \n",
    "#         dict_ES_pmi = pmi_to_dict_adj_dict(instance_ES_pmi)\n",
    "#         # print(\"ES finidhsed\")\n",
    "        \n",
    "#         # current_time = time.time()  # Get the current time\n",
    "#         # elapsed_time = current_time - start_time\n",
    "#         # start_time = current_time\n",
    "#         # print(elapsed_time)\n",
    "\n",
    "#         # print(\"KS\")\n",
    "#         KS_matrix, KS_dist, KS_Count_matrix = docs_distribution(baseSpace=KS, tE=tE)\n",
    "#         KS_size = list(range(KS_matrix.shape[0]))\n",
    "#         # print(\"KS finished\")\n",
    "\n",
    "#         # current_time = time.time()  # Get the current time\n",
    "#         # elapsed_time = current_time - start_time\n",
    "#         # start_time = current_time\n",
    "#         # print(elapsed_time)\n",
    "\n",
    "#         # print(\"Cluster\")\n",
    "#         if useClusters==True:\n",
    "#             KSClusterDiff1000 = ClusterKS(list_know_P=KS_matrix, new_Q= None, N=100, nbPtsPerCluster=1000)\n",
    "#             KSClusterDiff1000.clusterKS()\n",
    "#         # print(\"Cluster finished\")    \n",
    "#         # ES_size = list(range(ES_matrix.shape[0]))\n",
    "        \n",
    "#         # current_time = time.time()  # Get the current time\n",
    "#         # elapsed_time = current_time - start_time\n",
    "#         # start_time = current_time\n",
    "#         # print(elapsed_time)\n",
    "        \n",
    "#         ## We set to 0 the distance here for each recipe -- difference needs to estimate distance between all points. \n",
    "#         #This serves as optim to not calculate for each varaitions but only once since it is the same distance for all KB\n",
    "#         # print('train variations for recipe {} done'.format(recette), ' | train variation size : ', len(train_recettes))\n",
    "        \n",
    "neighborhood_distance  = 0.4\n",
    "new_ratio_vec = []\n",
    "new_bin_vec = []\n",
    "uniq_ratio_vec = []\n",
    "uniq_bin_vec = []\n",
    "diff_ratio_vec = []\n",
    "diff_bin_vec = []\n",
    "neighborhood_distance_vec = []\n",
    "surpDiv_ratio_vec = []\n",
    "surpDiv_bin_vec = []\n",
    "# mean100_vec = []\n",
    "\n",
    "for i in tqdm(range(7000, 8000)):\n",
    "    # print(\"new KS dist (with 1 toEval)\")\n",
    "    select_variation = KS_size + [len(KS_size)+i]\n",
    "    NewKS_dist, variation_dist = new_distribution(KS_Count_matrix, select_variation)\n",
    "    \n",
    "    # print(\"new ES pmi (with 1 toEval)\")\n",
    "    # # instance_ES_updated_pmi = copy.deepcopy(incremental_pmi_ES_only) # reprise du pmi du ES seulement\n",
    "    # new_pmi_instance = OptimizedIncrementalPMI(window_size = w_size)\n",
    "    # update_text = [word for text in tqdm(tE[i]) for word in text.split()] \n",
    "    # new_pmi_instance.update(update_text)\n",
    "    # new_pmi = new_pmi_instance.compute_pmi()\n",
    "\n",
    "    baseTexts_update = [tE[i]]\n",
    "    update_text = [word for text in (baseTexts_update) for word in text.split()]\n",
    "    new_pmi = OptimizedIncrementalPMI(window_size=3)\n",
    "    new_pmi.update(update_text)\n",
    "    newpmi_PMI = new_pmi.compute_pmi()\n",
    "    \n",
    "    # print(\"compute scores\")\n",
    "    results = compute_scores(KB_matrix=KS_matrix, KB_dist=KS_dist, NewKB_dist=NewKS_dist, variation_dist=variation_dist, \n",
    "                                EB_PMI=instance_ES_pmi, base_bigram_set=base_bigram_set, dict_know_pmi=dict_ES_pmi, New_EB_PMI=newpmi_PMI,\n",
    "                                neighbor_dist=neighborhood_distance, useClusters=True, KSCluster=KSClusterDiff1000)\n",
    "\n",
    "    new_ratio = results[0]\n",
    "    new_bin = results[1]\n",
    "\n",
    "    uniq_ratio = results[2]\n",
    "    uniq_bin = results[3]\n",
    "\n",
    "    diff_ratio = results[4]\n",
    "    diff_bin = results[5]\n",
    "    neighborhood_distance = results[6] #update neighborhood distance so it isn't reset\n",
    "    # mean100 = results[7]\n",
    "\n",
    "    # surpNew_ratio = results[7]\n",
    "    # surpNew_bin = results[8]\n",
    "\n",
    "    surpDiv_ratio = results[8]\n",
    "    surpDiv_bin = results[9]\n",
    "\n",
    "    new_ratio_vec.append(new_ratio)\n",
    "    new_bin_vec.append(new_bin)\n",
    "\n",
    "    uniq_ratio_vec.append(uniq_ratio)\n",
    "    uniq_bin_vec.append(uniq_bin)\n",
    "\n",
    "    diff_ratio_vec.append(diff_ratio)\n",
    "    diff_bin_vec.append(diff_bin)\n",
    "    neighborhood_distance_vec.append(neighborhood_distance)\n",
    "    # mean100_vec.append(mean100)\n",
    "\n",
    "    surpDiv_ratio_vec.append(surpDiv_ratio)\n",
    "    surpDiv_bin_vec.append(surpDiv_bin)\n",
    "\n",
    "    if i % 1000 == 0:\n",
    "        current_time = time.time()  # Get the current time\n",
    "        elapsed_time = current_time - start_time  # Calculate elapsed time\n",
    "        if i % 1000 == 0:\n",
    "            print(f\"{i} on {len(tE)}, time since last print: {elapsed_time:.2f} seconds\")\n",
    "        start_time = current_time  # Reset the start time for the next interval\n",
    "            # Reset the start time to measure time per iteration\n",
    "        start_time = time.time()\n",
    "\n",
    "# print(\"df\")\n",
    "\n",
    "\n",
    "start_time = current_time\n",
    "df = pd.DataFrame({\n",
    "    \"application_number\": application_number[:2],\n",
    "    \"label\": label[:2],\n",
    "\n",
    "    \"new_ratio\": new_ratio_vec,\n",
    "    \"new_bin\": new_bin_vec,\n",
    "\n",
    "    \"uniq_ratio\": uniq_ratio_vec,\n",
    "    \"uniq_bin\": uniq_bin_vec,\n",
    "\n",
    "    \"diff_ratio\": diff_ratio_vec,\n",
    "    \"diff_bin\": diff_bin_vec, \n",
    "    \"neighboroud_distance\": neighborhood_distance_vec,\n",
    "    # \"mean100\": mean100_vec\n",
    "\n",
    "    # \"surpNew_ratio\": surpNew_ratio,\n",
    "    # \"surpNew_bin\": surpNew_bin,\n",
    "\n",
    "    \"surpDiv_ratio\":surpDiv_ratio_vec,\n",
    "    \"surpDiv_bin\": surpDiv_bin_vec\n",
    "})\n",
    "# current_time = time.time()  # Get the current time\n",
    "# elapsed_time = current_time - start_time\n",
    "# print(elapsed_time)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
