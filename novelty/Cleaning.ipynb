{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install spacy\n",
    "# !python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.set_autosave_interval(5000)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosaving every 5 seconds\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import logging\n",
    "\n",
    "nblog = open(\"nb_clean.log\", \"a+\")\n",
    "sys.stdout.echo = nblog\n",
    "sys.stderr.echo = nblog\n",
    "\n",
    "get_ipython().log.handlers[0].stream = nblog\n",
    "get_ipython().log.setLevel(logging.INFO)\n",
    "\n",
    "%autosave 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/edgarlanoue/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/edgarlanoue/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "import re\n",
    "import ast\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter, defaultdict\n",
    "nltk.download('stopwords')\n",
    "import glob\n",
    "from textCleaning import parse_stopwords, get_file_names, extract_year_ipc\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/edgarlanoue/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import textCleaning\n",
    "importlib.reload(textCleaning)\n",
    "from textCleaning import parse_stopwords, get_file_names, extract_year_ipc, Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = \"C:/Users/edgar/OneDrive/Bureau/Ecole/HEC/A24/BrevetNLP/\"\n",
    "path = \"/home/edgarlanoue/data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "technet = pd.read_csv(path+\"Technet/clean_vocab.csv\")\n",
    "tn_lemm = pd.read_csv(path+\"Technet/lemmatized_technet.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length=  179 ,  Stopwords NLTK: ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
      "length=  99 ,  Stopwords USTPO: ['a', 'accordance', 'according', 'all', 'also', 'an', 'and', 'another', 'are', 'as', 'at', 'be', 'because', 'been', 'being', 'by', 'claim', 'comprises', 'corresponding', 'could', 'described', 'desired', 'do', 'does', 'each', 'embodiment', 'fig', 'figs', 'for', 'from', 'further', 'generally', 'had', 'has', 'have', 'having', 'herein', 'however', 'if', 'in', 'into', 'invention', 'is', 'it', 'its', 'means', 'not', 'now', 'of', 'on', 'onto', 'or', 'other', 'particularly', 'preferably', 'preferred', 'present', 'provide', 'provided', 'provides', 'relatively', 'respectively', 'said', 'should', 'since', 'some', 'such', 'suitable', 'than', 'that', 'the', 'their', 'then', 'there', 'thereby', 'therefore', 'thereof', 'thereto', 'these', 'they', 'this', 'those', 'thus', 'to', 'use', 'various', 'was', 'were', 'what', 'when', 'where', 'whereby', 'wherein', 'which', 'while', 'who', 'will', 'with', 'would']\n",
      "length=  87 ,  Stopwords TN_technical: ['able', 'above-mentioned', 'accordingly', 'across', 'along', 'already', 'alternatively', 'always', 'among', 'and/or', 'anything', 'anywhere', 'better', 'disclosure', 'due', 'easily', 'easy', 'eg', 'either', 'elsewhere', 'enough', 'especially', 'essentially', 'et al', 'etc', 'eventually', 'excellent', 'finally', 'furthermore', 'good', 'hence', 'he/she', 'him/her', 'his/her', 'ie', 'ii', 'iii', 'instead', 'later', 'like', 'little', 'many', 'may', 'meanwhile', 'might', 'moreover', 'much', 'must', 'never', 'often', 'others', 'otherwise', 'overall', 'rather', 'remarkably', 'significantly', 'simply', 'sometimes', 'specifically', 'straight forward', 'substantially', 'thereafter', 'therebetween', 'therefor', 'therefrom', 'therein', 'thereinto', 'thereon', 'therethrough', 'therewith', 'together', 'toward', 'towards', 'typical', 'typically', 'upon', 'via', 'vice versa', 'whatever', 'whereas', 'whereat', 'wherever', 'whether', 'whose', 'within', 'without', 'yet']\n",
      "length=  47 ,  toAdd: ['or', 'in', 'be', 'but ', 'an', 'of', 'may', 'as', 'soh', 'eoh', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '1', '2', '3', '4', '5', '6', '7', '8', '9', '0', '-']\n",
      "length=  32 ,  newSW: ['first', 'second', 'method', 'apparatus', 'step', '10', '/', 'image', 'third', '11', '12', 'point', '14', 'application', '13', '15', 'display', 'section', 'current', 'c1', 'ue', '20', '16', 'show', '17', '18', 'I', 'art', '19', 'summary', 'patent', 'example']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "file_path = \"/home/edgarlanoue/brevetNLP/technet/stopwords.txt\" #path+\"PatentNovelty/technet/stopwords.txt\" \n",
    "sW_dict = parse_stopwords(file_path)\n",
    "\n",
    "# Access each list by its label\n",
    "stopwords_nltk = sW_dict.get(\"Stopwords NLTK\", [])\n",
    "stopwords_ustpo = sW_dict.get(\"Stopwords USTPO\", [])\n",
    "stopwords_tn_technical = sW_dict.get(\"Stopwords TN_technical\", [])\n",
    "to_add = sW_dict.get(\"toAdd\", [])\n",
    "new_sw = sW_dict.get(\"newSW\", [])\n",
    "\n",
    "# Print the lists to verify\n",
    "print(\"length= \", len(stopwords_nltk), \", \", \"Stopwords NLTK:\", stopwords_nltk)\n",
    "print(\"length= \", len(stopwords_ustpo), \", \", \"Stopwords USTPO:\", stopwords_ustpo)\n",
    "print(\"length= \", len(stopwords_tn_technical), \", \", \"Stopwords TN_technical:\", stopwords_tn_technical)\n",
    "print(\"length= \", len(to_add), \", \", \"toAdd:\", to_add)\n",
    "print(\"length= \", len(new_sw), \", \", \"newSW:\", new_sw)\n",
    "\n",
    "stopwords = list(set([item for sublist in sW_dict.values() for item in sublist]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab=textCleaning.Vocab(technet=technet, df_lemm=tn_lemm, stopwords=stopwords)\n",
    "# len(vocab.tn_lemm_filtered) # 845647"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/home/edgarlanoue/data/csv/\"  #\"C:/Users/edgar/OneDrive/Bureau/Ecole/HEC/A24/BrevetNLP/exemple données/test/\"\n",
    "pathOutput = \"/home/edgarlanoue/data/csvCleaned/\" #\"C:/Users/edgar/OneDrive/Bureau/Ecole/HEC/A24/BrevetNLP/exemple données/cleaned/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanCSV(path, pathOutput, yearList, ipcList):\n",
    "    tE_names = get_file_names(path+\"toEval\")\n",
    "    KS_names = get_file_names(path+\"KS\")\n",
    "    ES_names = get_file_names(path+\"ES\")\n",
    "\n",
    "    tE_set = set([extract_year_ipc(string) for string in tE_names])\n",
    "    KS_set = set([extract_year_ipc(string) for string in KS_names])\n",
    "    ES_set = set([extract_year_ipc(string) for string in ES_names])\n",
    "\n",
    "    assert(tE_set==KS_set==ES_set)\n",
    "\n",
    "    if yearList != None and ipcList != None:\n",
    "        yearList = yearList\n",
    "        ipcList = ipcList\n",
    "    else:\n",
    "        yearList = [year for year, ipc in list(tE_set)]\n",
    "        ipcList = [ipc for year, ipc in list(tE_set)]\n",
    "\n",
    "    for year in yearList:\n",
    "        for ipc in ipcList:\n",
    "            print(f\"{year}\")\n",
    "            print(f\"    {ipc}\")\n",
    "            \n",
    "            # tE = pd.read_csv(path+f\"exemple données/test/toEval/{year}_{ipc}_patents_toEval.csv\")\n",
    "            # matching_files = glob.glob(path+f\"exemple données/test/KS/{year}_*_{ipc}_KS_raw.csv\")[0]\n",
    "            # KS = pd.read_csv(matching_files)\n",
    "            # matching_files = glob.glob(path+f\"exemple données/test/ES/{year}_*_{ipc}_ES_raw.csv\")[0]\n",
    "            # ES = pd.read_csv(matching_files)\n",
    "\n",
    "            tE = pd.read_csv(path+f\"toEval/{year}_{ipc}_patents_toEval.csv\")\n",
    "            matching_files = glob.glob(path+f\"KS/{year}_*_{ipc}_KS_raw.csv\")[0]\n",
    "            KS = pd.read_csv(matching_files)\n",
    "            matching_files = glob.glob(path+f\"ES/{year}_*_{ipc}_ES_raw.csv\")[0]\n",
    "            ES = pd.read_csv(matching_files)\n",
    "\n",
    "            tE_clean = vocab.lemmDF(vocab.cleanDF(tE, type=\"all\"))\n",
    "            KS_clean = vocab.lemmDF(vocab.cleanDF(KS, type=\"all\"))\n",
    "            ES_clean = vocab.lemmDF(vocab.cleanDF(ES, type=\"all\"))\n",
    "\n",
    "            # tE_clean.to_csv(path+f\"exemple données/cleaned/tE/{year}_{ipc}_tE_cleaned.csv\", index=False)\n",
    "            # KS_clean.to_csv(path+f\"exemple données/cleaned/KS/{year}_{ipc}_KS_cleaned.csv\", index=False)\n",
    "            # ES_clean.to_csv(path+f\"exemple données/cleaned/ES/{year}_{ipc}_ES_cleaned.csv\", index=False)\n",
    "\n",
    "            tE_clean.to_csv(pathOutput+f\"/tE/{year}_{ipc}_tE_cleaned.csv\", index=False)\n",
    "            KS_clean.to_csv(pathOutput+f\"/KS/{year}_{ipc}_KS_cleaned.csv\", index=False)\n",
    "            ES_clean.to_csv(pathOutput+f\"/ES/{year}_{ipc}_ES_cleaned.csv\", index=False)\n",
    "\n",
    "    return None\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2013\n",
      "    H01L\n",
      "Clean\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20359/20359 [00:41<00:00, 491.31it/s]\n",
      "  0%|          | 0/20359 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "yearList = ['2013', '2014', '2015']\n",
    "ipcList = [\"H01L\", \"A61B\"]\n",
    "\n",
    "cleanCSV(path, pathOutput, yearList, ipcList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yearList = ['2012', '2013', '2014', '2015', '2016']\n",
    "ipcList = [\"G01N\", \"B60L\", \"E21B\", \"F03D\", \"H04W\", \"C07D\", \"D07B\", \"B32B\"]\n",
    "\n",
    "cleanCSV(path, pathOutput, yearList, ipcList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_tE = path+\"toEval\" # exemple données/test/\n",
    "path_KS = path+\"KS\" #exemple données/test/\n",
    "path_ES = path+\"ES\" #exemple données/test/\n",
    "\n",
    "tE_names = get_file_names(path_tE)\n",
    "KS_names = get_file_names(path_KS)\n",
    "ES_names = get_file_names(path_ES)\n",
    "\n",
    "tE_set = set([extract_year_ipc(string) for string in tE_names])\n",
    "KS_set = set([extract_year_ipc(string) for string in KS_names])\n",
    "ES_set = set([extract_year_ipc(string) for string in ES_names])\n",
    "\n",
    "assert(tE_set==KS_set==ES_set)\n",
    "\n",
    "yearList = sorted(list(set([year for year, ipc in list(tE_set)])))\n",
    "ipcList = list(set([ipc for year, ipc in list(tE_set)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2011', '2012', '2013', '2014', '2015', '2016']\n",
      "['G01N', 'E21B', 'F03D', 'B60L', 'B32B', 'G06F', 'H04W', 'C07D', 'D07B', 'A61B', 'H01L']\n"
     ]
    }
   ],
   "source": [
    "print(yearList)\n",
    "print(ipcList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "yearList = ['2012', '2013', '2014', '2015', '2016']\n",
    "ipcList = [\"A61B\", \"H01L\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "yearList = ['2016']#, '2013', '2014', '2015', '2016']\n",
    "ipcList = [\"H01L\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016\n",
      "    H01L\n"
     ]
    }
   ],
   "source": [
    "for year in yearList:\n",
    "    for ipc in ipcList:\n",
    "        print(f\"{year}\")\n",
    "        print(f\"    {ipc}\")\n",
    "        tE = pd.read_csv(path+f\"toEval/{year}_{ipc}_patents_toEval.csv\")\n",
    "        matching_files = glob.glob(path+f\"KS/{year}_*_{ipc}_KS_raw.csv\")[0]\n",
    "        KS = pd.read_csv(matching_files)\n",
    "        matching_files = glob.glob(path+f\"ES/{year}_*_{ipc}_ES_raw.csv\")[0]\n",
    "        ES = pd.read_csv(matching_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tE_clean = vocab.lemmDF(vocab.cleanDF(tE, type=\"all\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40960/40960 [01:23<00:00, 491.69it/s]\n",
      "100%|██████████| 40960/40960 [03:32<00:00, 192.40it/s]\n",
      "100%|██████████| 40960/40960 [00:18<00:00, 2236.86it/s]\n",
      "100%|██████████| 40960/40960 [01:45<00:00, 389.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatize\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40960/40960 [00:05<00:00, 8102.51it/s]\n",
      "100%|██████████| 40960/40960 [00:11<00:00, 3547.31it/s]\n",
      "100%|██████████| 40960/40960 [00:01<00:00, 29702.66it/s]\n",
      "100%|██████████| 40960/40960 [00:06<00:00, 6396.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 173135/173135 [06:39<00:00, 433.60it/s]\n",
      "100%|██████████| 173135/173135 [13:58<00:00, 206.52it/s]\n",
      "100%|██████████| 173135/173135 [01:22<00:00, 2105.42it/s]\n",
      "100%|██████████| 173135/173135 [07:18<00:00, 395.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatize\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 173135/173135 [00:25<00:00, 6851.80it/s]\n",
      "100%|██████████| 173135/173135 [00:48<00:00, 3562.81it/s]\n",
      "100%|██████████| 173135/173135 [00:06<00:00, 28630.01it/s]\n",
      "100%|██████████| 173135/173135 [00:28<00:00, 6072.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 396708/396708 [20:11<00:00, 327.51it/s]\n",
      "100%|██████████| 396708/396708 [32:59<00:00, 200.37it/s] \n",
      "100%|██████████| 396708/396708 [03:36<00:00, 1830.38it/s]\n",
      "100%|██████████| 396708/396708 [23:22<00:00, 282.86it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatize\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 396708/396708 [01:12<00:00, 5494.31it/s]\n",
      "100%|██████████| 396708/396708 [01:51<00:00, 3552.05it/s]\n",
      "100%|██████████| 396708/396708 [00:14<00:00, 28097.29it/s]\n",
      "100%|██████████| 396708/396708 [01:18<00:00, 5081.83it/s]\n"
     ]
    }
   ],
   "source": [
    "tE_clean = vocab.lemmDF(vocab.cleanDF(tE, type=\"all\"))\n",
    "KS_clean = vocab.lemmDF(vocab.cleanDF(KS, type=\"all\"))\n",
    "ES_clean = vocab.lemmDF(vocab.cleanDF(ES, type=\"all\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tE_clean.to_csv(path+f\"/home/edgarlanoue/data/csvCleaned/tE/{year}_{ipc}_tE_cleaned.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/edgarlanoue/data/csvCleaned/'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pathOutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "tE_clean.to_csv(pathOutput+f\"/tE/{year}_{ipc}_tE_cleaned.csv\", index=False)\n",
    "KS_clean.to_csv(pathOutput+f\"/KS/{year}_{ipc}_KS_cleaned.csv\", index=False)\n",
    "ES_clean.to_csv(pathOutput+f\"/ES/{year}_{ipc}_ES_cleaned.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vérification\n",
    "# count_word_frequency_total(clean_KS_H01L_2016) - c'est bon"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10",
   "language": "python",
   "name": "python3.10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
