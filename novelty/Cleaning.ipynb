{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install spacy\n",
    "# !python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# import logging\n",
    "\n",
    "# nblog = open(\"nb_freqWords.log\", \"a+\")\n",
    "# sys.stdout.echo = nblog\n",
    "# sys.stderr.echo = nblog\n",
    "\n",
    "# get_ipython().log.handlers[0].stream = nblog\n",
    "# get_ipython().log.setLevel(logging.INFO)\n",
    "\n",
    "# %autosave 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\edgar\\OneDrive\\Bureau\\Ecole\\HEC\\A24\\BrevetNLP\\.conda\\Lib\\site-packages\\spacy\\util.py:910: UserWarning: [W095] Model 'en_core_web_sm' (3.8.0) was trained with spaCy v3.8.0 and may not be 100% compatible with the current version (3.7.5). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\edgar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "c:\\Users\\edgar\\OneDrive\\Bureau\\Ecole\\HEC\\A24\\BrevetNLP\\.conda\\Lib\\site-packages\\spacy\\util.py:910: UserWarning: [W095] Model 'en_core_web_sm' (3.8.0) was trained with spaCy v3.8.0 and may not be 100% compatible with the current version (3.7.5). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\edgar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "import re\n",
    "import ast\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter, defaultdict\n",
    "nltk.download('stopwords')\n",
    "import glob\n",
    "from textCleaning import parse_stopwords, get_file_names, extract_year_ipc\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\edgar\\OneDrive\\Bureau\\Ecole\\HEC\\A24\\BrevetNLP\\.conda\\Lib\\site-packages\\spacy\\util.py:910: UserWarning: [W095] Model 'en_core_web_sm' (3.8.0) was trained with spaCy v3.8.0 and may not be 100% compatible with the current version (3.7.5). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\edgar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import textCleaning\n",
    "importlib.reload(textCleaning)\n",
    "from textCleaning import parse_stopwords, get_file_names, extract_year_ipc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"C:/Users/edgar/OneDrive/Bureau/Ecole/HEC/A24/BrevetNLP/\"\n",
    "# path = \"/home/edgarlanoue/data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "technet = pd.read_csv(path+\"Technet/clean_vocab.csv\")\n",
    "tn_lemm = pd.read_csv(path+\"Technet/lemmatized_technet.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length=  179 ,  Stopwords NLTK: ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
      "length=  99 ,  Stopwords USTPO: ['a', 'accordance', 'according', 'all', 'also', 'an', 'and', 'another', 'are', 'as', 'at', 'be', 'because', 'been', 'being', 'by', 'claim', 'comprises', 'corresponding', 'could', 'described', 'desired', 'do', 'does', 'each', 'embodiment', 'fig', 'figs', 'for', 'from', 'further', 'generally', 'had', 'has', 'have', 'having', 'herein', 'however', 'if', 'in', 'into', 'invention', 'is', 'it', 'its', 'means', 'not', 'now', 'of', 'on', 'onto', 'or', 'other', 'particularly', 'preferably', 'preferred', 'present', 'provide', 'provided', 'provides', 'relatively', 'respectively', 'said', 'should', 'since', 'some', 'such', 'suitable', 'than', 'that', 'the', 'their', 'then', 'there', 'thereby', 'therefore', 'thereof', 'thereto', 'these', 'they', 'this', 'those', 'thus', 'to', 'use', 'various', 'was', 'were', 'what', 'when', 'where', 'whereby', 'wherein', 'which', 'while', 'who', 'will', 'with', 'would']\n",
      "length=  87 ,  Stopwords TN_technical: ['able', 'above-mentioned', 'accordingly', 'across', 'along', 'already', 'alternatively', 'always', 'among', 'and/or', 'anything', 'anywhere', 'better', 'disclosure', 'due', 'easily', 'easy', 'eg', 'either', 'elsewhere', 'enough', 'especially', 'essentially', 'et al', 'etc', 'eventually', 'excellent', 'finally', 'furthermore', 'good', 'hence', 'he/she', 'him/her', 'his/her', 'ie', 'ii', 'iii', 'instead', 'later', 'like', 'little', 'many', 'may', 'meanwhile', 'might', 'moreover', 'much', 'must', 'never', 'often', 'others', 'otherwise', 'overall', 'rather', 'remarkably', 'significantly', 'simply', 'sometimes', 'specifically', 'straight forward', 'substantially', 'thereafter', 'therebetween', 'therefor', 'therefrom', 'therein', 'thereinto', 'thereon', 'therethrough', 'therewith', 'together', 'toward', 'towards', 'typical', 'typically', 'upon', 'via', 'vice versa', 'whatever', 'whereas', 'whereat', 'wherever', 'whether', 'whose', 'within', 'without', 'yet']\n",
      "length=  47 ,  toAdd: ['or', 'in', 'be', 'but ', 'an', 'of', 'may', 'as', 'soh', 'eoh', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '1', '2', '3', '4', '5', '6', '7', '8', '9', '0', '-']\n",
      "length=  32 ,  newSW: ['first', 'second', 'method', 'apparatus', 'step', '10', '/', 'image', 'third', '11', '12', 'point', '14', 'application', '13', '15', 'display', 'section', 'current', 'c1', 'ue', '20', '16', 'show', '17', '18', 'I', 'art', '19', 'summary', 'patent', 'example']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "file_path = path+\"PatentNovelty/technet/stopwords.txt\" \n",
    "sW_dict = parse_stopwords(file_path)\n",
    "\n",
    "# Access each list by its label\n",
    "stopwords_nltk = sW_dict.get(\"Stopwords NLTK\", [])\n",
    "stopwords_ustpo = sW_dict.get(\"Stopwords USTPO\", [])\n",
    "stopwords_tn_technical = sW_dict.get(\"Stopwords TN_technical\", [])\n",
    "to_add = sW_dict.get(\"toAdd\", [])\n",
    "new_sw = sW_dict.get(\"newSW\", [])\n",
    "\n",
    "# Print the lists to verify\n",
    "print(\"length= \", len(stopwords_nltk), \", \", \"Stopwords NLTK:\", stopwords_nltk)\n",
    "print(\"length= \", len(stopwords_ustpo), \", \", \"Stopwords USTPO:\", stopwords_ustpo)\n",
    "print(\"length= \", len(stopwords_tn_technical), \", \", \"Stopwords TN_technical:\", stopwords_tn_technical)\n",
    "print(\"length= \", len(to_add), \", \", \"toAdd:\", to_add)\n",
    "print(\"length= \", len(new_sw), \", \", \"newSW:\", new_sw)\n",
    "\n",
    "stopwords = list(set([item for sublist in sW_dict.values() for item in sublist]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab=textCleaning.Vocab(technet=technet, df_lemm=tn_lemm, stopwords=stopwords)\n",
    "# len(vocab.tn_lemm_filtered) # 845647"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"C:/Users/edgar/OneDrive/Bureau/Ecole/HEC/A24/BrevetNLP/exemple données/test/\"\n",
    "pathOutput = \"C:/Users/edgar/OneDrive/Bureau/Ecole/HEC/A24/BrevetNLP/exemple données/cleaned/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanCSV(path, pathOutput):\n",
    "    tE_names = get_file_names(path+\"toEval\")\n",
    "    KS_names = get_file_names(path+\"KS\")\n",
    "    ES_names = get_file_names(path+\"ES\")\n",
    "\n",
    "    tE_set = set([extract_year_ipc(string) for string in tE_names])\n",
    "    KS_set = set([extract_year_ipc(string) for string in KS_names])\n",
    "    ES_set = set([extract_year_ipc(string) for string in ES_names])\n",
    "\n",
    "    assert(tE_set==KS_set==ES_set)\n",
    "\n",
    "    yearList = [year for year, ipc in list(tE_set)]\n",
    "    ipcList = [ipc for year, ipc in list(tE_set)]\n",
    "\n",
    "    for year in yearList:\n",
    "        for ipc in ipcList:\n",
    "            print(f\"{year}\")\n",
    "            print(f\"    {ipc}\")\n",
    "            tE = pd.read_csv(path+f\"exemple données/test/toEval/{year}_{ipc}_patents_toEval.csv\")\n",
    "            matching_files = glob.glob(path+f\"exemple données/test/KS/{year}_*_{ipc}_KS_raw.csv\")[0]\n",
    "            KS = pd.read_csv(matching_files)\n",
    "            matching_files = glob.glob(path+f\"exemple données/test/ES/{year}_*_{ipc}_ES_raw.csv\")[0]\n",
    "            ES = pd.read_csv(matching_files)\n",
    "\n",
    "            tE_clean = vocab.lemmDF(vocab.cleanDF(tE, type=\"all\"))\n",
    "            KS_clean = vocab.lemmDF(vocab.cleanDF(KS, type=\"all\"))\n",
    "            ES_clean = vocab.lemmDF(vocab.cleanDF(ES, type=\"all\"))\n",
    "\n",
    "            tE_clean.to_csv(path+f\"exemple données/cleaned/tE/{year}_{ipc}_tE_cleaned.csv\", index=False)\n",
    "            KS_clean.to_csv(path+f\"exemple données/cleaned/KS/{year}_{ipc}_KS_cleaned.csv\", index=False)\n",
    "            ES_clean.to_csv(path+f\"exemple données/cleaned/ES/{year}_{ipc}_ES_cleaned.csv\", index=False)\n",
    "\n",
    "    return None\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_tE = path+\"exemple données/test/toEval\"\n",
    "path_KS = path+\"exemple données/test/KS\"\n",
    "path_ES = path+\"exemple données/test/ES\"\n",
    "\n",
    "tE_names = get_file_names(path_tE)\n",
    "KS_names = get_file_names(path_KS)\n",
    "ES_names = get_file_names(path_ES)\n",
    "\n",
    "tE_set = set([extract_year_ipc(string) for string in tE_names])\n",
    "KS_set = set([extract_year_ipc(string) for string in KS_names])\n",
    "ES_set = set([extract_year_ipc(string) for string in ES_names])\n",
    "\n",
    "assert(tE_set==KS_set==ES_set)\n",
    "\n",
    "yearList = [year for year, ipc in list(tE_set)]\n",
    "ipcList = [ipc for year, ipc in list(tE_set)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016\n",
      "    H01L\n"
     ]
    }
   ],
   "source": [
    "for year, ipc in zip(yearList, ipcList):\n",
    "    print(f\"{year}\")\n",
    "    print(f\"    {ipc}\")\n",
    "    tE = pd.read_csv(path+f\"exemple données/test/toEval/{year}_{ipc}_patents_toEval.csv\")\n",
    "    matching_files = glob.glob(path+f\"exemple données/test/KS/{year}_*_{ipc}_KS_raw.csv\")[0]\n",
    "    KS = pd.read_csv(matching_files)\n",
    "    matching_files = glob.glob(path+f\"exemple données/test/ES/{year}_*_{ipc}_ES_raw.csv\")[0]\n",
    "    ES = pd.read_csv(matching_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11700/11700 [00:10<00:00, 1160.84it/s]\n",
      "100%|██████████| 11700/11700 [00:19<00:00, 597.66it/s]\n",
      "100%|██████████| 11700/11700 [00:02<00:00, 4304.15it/s]\n",
      "100%|██████████| 11700/11700 [00:15<00:00, 767.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatize\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11700/11700 [00:00<00:00, 12738.53it/s]\n",
      "100%|██████████| 11700/11700 [00:01<00:00, 5858.04it/s]\n",
      "100%|██████████| 11700/11700 [00:00<00:00, 41709.82it/s]\n",
      "100%|██████████| 11700/11700 [00:01<00:00, 7436.56it/s]\n"
     ]
    }
   ],
   "source": [
    "tE_clean = vocab.lemmDF(vocab.cleanDF(tE, type=\"all\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11700/11700 [00:10<00:00, 1139.58it/s]\n",
      "100%|██████████| 11700/11700 [00:25<00:00, 463.23it/s]\n",
      "100%|██████████| 11700/11700 [00:02<00:00, 4164.01it/s]\n",
      "100%|██████████| 11700/11700 [00:16<00:00, 689.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatize\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11700/11700 [00:00<00:00, 13512.41it/s]\n",
      "100%|██████████| 11700/11700 [00:02<00:00, 5720.91it/s]\n",
      "100%|██████████| 11700/11700 [00:00<00:00, 40028.97it/s]\n",
      "100%|██████████| 11700/11700 [00:01<00:00, 7072.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 99087/99087 [02:05<00:00, 791.09it/s] \n",
      "100%|██████████| 99087/99087 [03:47<00:00, 435.27it/s]\n",
      "100%|██████████| 99087/99087 [00:22<00:00, 4364.74it/s]\n",
      "100%|██████████| 99087/99087 [02:31<00:00, 653.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatize\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 99087/99087 [00:09<00:00, 10080.62it/s]\n",
      "100%|██████████| 99087/99087 [00:18<00:00, 5215.92it/s]\n",
      "100%|██████████| 99087/99087 [00:02<00:00, 40330.40it/s]\n",
      "100%|██████████| 99087/99087 [00:13<00:00, 7435.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 284544/284544 [06:55<00:00, 684.09it/s] \n",
      "100%|██████████| 284544/284544 [13:19<00:00, 355.89it/s]\n",
      "100%|██████████| 284544/284544 [01:16<00:00, 3727.48it/s]\n",
      "100%|██████████| 284544/284544 [09:09<00:00, 517.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatize\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 284544/284544 [00:51<00:00, 5522.43it/s]\n",
      "100%|██████████| 284544/284544 [01:21<00:00, 3483.58it/s]\n",
      "100%|██████████| 284544/284544 [00:08<00:00, 32758.36it/s]\n",
      "100%|██████████| 284544/284544 [00:48<00:00, 5825.28it/s]\n"
     ]
    }
   ],
   "source": [
    "tE_clean = vocab.lemmDF(vocab.cleanDF(tE, type=\"all\"))\n",
    "KS_clean = vocab.lemmDF(vocab.cleanDF(KS, type=\"all\"))\n",
    "ES_clean = vocab.lemmDF(vocab.cleanDF(ES, type=\"all\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "tE_clean.to_csv(path+f\"exemple données/cleaned/tE/{year}_{ipc}_tE_cleaned.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tE_clean.to_csv(path+f\"exemple données/cleaned/tE/{year}_{ipc}_tE_cleaned.csv\", index=False)\n",
    "KS_clean.to_csv(path+f\"exemple données/cleaned/KS/{year}_{ipc}_KS_cleaned.csv\", index=False)\n",
    "ES_clean.to_csv(path+f\"exemple données/cleaned/ES/{year}_{ipc}_ES_cleaned.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vérification\n",
    "# count_word_frequency_total(clean_KS_H01L_2016) - c'est bon"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
